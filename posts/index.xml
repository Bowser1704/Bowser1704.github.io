<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on Bowser&#39;s blog</title>
		<link>https://bowser1704.github.io/posts/</link>
		<description>Recent content in Posts on Bowser&#39;s blog</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
		<lastBuildDate>Sun, 08 Nov 2020 22:18:18 +0800</lastBuildDate>
		<atom:link href="https://bowser1704.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>k3s authentication certificate</title>
			<link>https://bowser1704.github.io/posts/k3s-auth/</link>
			<pubDate>Sun, 08 Nov 2020 22:18:18 +0800</pubDate>
			
			<guid>https://bowser1704.github.io/posts/k3s-auth/</guid>
			<description>k3s authentication 方式 client certificate token username and password certificate 在 k8s 的世界里面有两种证书，一种是 client certificate 用于认证，一种是 server certificate 用于 TLS 验证。 用于认证的就是 kubeconfig 文件里面的 client-certificate-data 和 client-key-data apiVersion:v1clusters:- cluster:certificate-authority-data:DATA+OMITTEDserver:https://kubernetes.docker.internal:6443name:docker-desktopcontexts:- context:cluster:docker-desktopuser:docker-desktopname:docker-desktop- context:cluster:docker-desktopuser:docker-desktopname:docker-for-desktopcurrent-context:docker-desktopkind:Configpreferences:{}users:- name:docker-desktopuser:client-certificate-data:REDACTEDclient-key-data:REDACTED client-certificate-data 证书文</description>
			<content type="html"><![CDATA[<h2 id="k3s-authentication-方式">k3s authentication 方式</h2>
<ul>
<li>client certificate</li>
<li>token</li>
<li>username and password</li>
</ul>
<h2 id="certificate">certificate</h2>
<p>在 k8s 的世界里面有两种证书，一种是 client certificate 用于认证，一种是 server certificate 用于 TLS 验证。</p>
<p>用于认证的就是 kubeconfig 文件里面的 client-certificate-data 和 client-key-data</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">clusters</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">cluster</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">certificate-authority-data</span><span class="p">:</span><span class="w"> </span><span class="l">DATA+OMITTED</span><span class="w">
</span><span class="w">    </span><span class="nt">server</span><span class="p">:</span><span class="w"> </span><span class="l">https://kubernetes.docker.internal:6443</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">docker-desktop</span><span class="w">
</span><span class="w"></span><span class="nt">contexts</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">context</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">cluster</span><span class="p">:</span><span class="w"> </span><span class="l">docker-desktop</span><span class="w">
</span><span class="w">    </span><span class="nt">user</span><span class="p">:</span><span class="w"> </span><span class="l">docker-desktop</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">docker-desktop</span><span class="w">
</span><span class="w"></span>- <span class="nt">context</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">cluster</span><span class="p">:</span><span class="w"> </span><span class="l">docker-desktop</span><span class="w">
</span><span class="w">    </span><span class="nt">user</span><span class="p">:</span><span class="w"> </span><span class="l">docker-desktop</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">docker-for-desktop</span><span class="w">
</span><span class="w"></span><span class="nt">current-context</span><span class="p">:</span><span class="w"> </span><span class="l">docker-desktop</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Config</span><span class="w">
</span><span class="w"></span><span class="nt">preferences</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span><span class="w"></span><span class="nt">users</span><span class="p">:</span><span class="w">
</span><span class="w"></span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">docker-desktop</span><span class="w">
</span><span class="w">  </span><span class="nt">user</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">client-certificate-data</span><span class="p">:</span><span class="w"> </span><span class="l">REDACTED</span><span class="w">
</span><span class="w">    </span><span class="nt">client-key-data</span><span class="p">:</span><span class="w"> </span><span class="l">REDACTED</span><span class="w">
</span></code></pre></div><ul>
<li>
<p>client-certificate-data</p>
<p>证书文件，里面包含 public key 和 signature，在 k3s 中由 client-ca 签发。</p>
</li>
<li>
<p>client-key-data</p>
<p>是上面 public key 对应的 private key，也是 client certificate 这种认证方式所需要的。</p>
</li>
<li>
<p>certificate-authority-data</p>
<p>这是 ca.crt 也就是 ca 的根证书，在 k3s 中这个是 server-ca 签发。也就是 server-ca 的证书。这个用于 TLS，也就是我们系统内置的根证书，它签发了一些服务端的证书。</p>
</li>
</ul>
<blockquote>
<p>k3s 中分了两个 ca，一个是 server-ca，一个是 client-ca，这不是必须的，可以只有一个 ca 签发所有证书。</p>
</blockquote>
<h3 id="tls-验证">TLS 验证</h3>
<p>在 k3s 中，有两个证书用于 HTTPS 验证，</p>
<ul>
<li>
<p>serving-kube-apiserver.crt</p>
<p><code>/var/lib/rancher/k3s/server/tls</code>  中的 serving-kube-apiserver.crt 这个用于 apiserver 内部组件认证。</p>
<p>这个证书当我们重启机器的时候 k3s 会有一个 rotation。</p>
<p>参考 pull request <a href="https://github.com/rancher/k3s/pull/1855#issuecomment-637704645">k3s#1855</a><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<blockquote>
<p>Yes, this controls the certificate that&rsquo;s used for the internal apiserver endpoint. As far as I can tell this is NOT the same endpoint as external clients (kubectl, etc) interact with on port 6443.</p>
</blockquote>
</li>
<li>
<p>k3s-serving</p>
<p>kube-system namespace 下面的 secret k3s-serving，用于外部访问，例如 <code>kubectl</code>，或者 <code>curl</code> 之类的。</p>
<p>经过检验，在我们部署的 pod 内部访问依旧是返回的这个 cert。</p>
</li>
</ul>
<p>issue <a href="https://github.com/rancher/k3s/issues/1621">k3s#1621</a><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> 解释了关于证书的更新。重启一下 k3s，就会自动更新所有证书（如果 Expiration &lt; 60d）。</p>
<p><strong>并且可能是 bug 只有 <a href="https://github.com/rancher/k3s/releases/tag/v1.19.3%2Bk3s2">v1.19.3+k3s2</a><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> 版本之后，才会更新 k3s-serving, 否则只会更新 <code>/var/lib/rancher/k3s/server/tls</code> 下的证书。</strong></p>
<h3 id="serviceaccount">serviceaccount</h3>
<p>在 k3s 的 serviceaccout 中，默认的 secret 也就是 xxxx-token-xxx，里面的信息有三部分。</p>
<ul>
<li>ca.crt 也就是 server-ca.crt 用于 TLS 验证，也就是一般我们系统内置的根证书。</li>
<li>token 就是认证信息</li>
<li>namespace  也就是这个 sa 的 ns。</li>
</ul>
<h3 id="references"><em>References</em></h3>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://github.com/rancher/k3s/pull/1855#issuecomment-637704645">https://github.com/rancher/k3s/pull/1855#issuecomment-637704645</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://github.com/rancher/k3s/issues/1621">https://github.com/rancher/k3s/issues/1621</a> <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://github.com/rancher/k3s/releases/tag/v1.19.3%2Bk3s2">https://github.com/rancher/k3s/releases/tag/v1.19.3%2Bk3s2</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content>
		</item>
		
		<item>
			<title>单机 Prometheus 监控多集群方案</title>
			<link>https://bowser1704.github.io/posts/cluster-monitoring/</link>
			<pubDate>Wed, 01 Jul 2020 22:21:01 +0800</pubDate>
			
			<guid>https://bowser1704.github.io/posts/cluster-monitoring/</guid>
			<description>初始化：安装各种组件 Prometheus 和 Alertmanager 的安装都在官网 下载压缩包，手动下载下来解压。 # -C 指定目的目录 # * 代表版本，需要下载正确的系统版本。 tar xf prometheus-*.*.*.linux-amd64.tar.gz -C /opt/ tar xf alertmanager-*.*.*.linux-amd64.tar.gz</description>
			<content type="html"><![CDATA[<h2 id="初始化安装各种组件">初始化：安装各种组件</h2>
<p>Prometheus 和 Alertmanager 的安装都在<a href="https://prometheus.io/download/">官网</a> 下载压缩包，手动下载下来解压。</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># -C 指定目的目录</span>
<span class="c1"># * 代表版本，需要下载正确的系统版本。</span>
tar xf prometheus-*.*.*.linux-amd64.tar.gz -C /opt/
tar xf alertmanager-*.*.*.linux-amd64.tar.gz -C /opt/
</code></pre></div><p>安装之后使用 systemd 来管理这几个服务。手写 service 放到 systemd 文件夹下面，我的目标是 <code>/usr/lib/systemd/system/</code> 。</p>
<p>prometheus 可以热加载配置文件，如果需要通过 web API 重启，必须要加参数 <code>--web.enable-lifecycle</code>，不安全，不推荐这么做。我们使用 systemd 管理 service，可以直接用 restart 命令重启。</p>
<p>注意我新建了两个配置文件，名字都为 config.yaml。</p>
<div class="highlight"><pre class="chroma"><code class="language-service" data-lang="service"><span class="k">[Unit]</span>
<span class="na">Description</span><span class="o">=</span><span class="s">Prometheus Server</span>
<span class="na">Wants</span><span class="o">=</span><span class="s">network-online.target</span>
<span class="na">After</span><span class="o">=</span><span class="s">network-online.target</span>

<span class="k">[Service]</span>
<span class="na">Type</span><span class="o">=</span><span class="s">simple</span>
<span class="na">ExecStartPre</span><span class="o">=</span><span class="s">/opt/prometheus/promtool check config /opt/prometheus/config.yaml</span>
<span class="na">ExecStart</span><span class="o">=</span><span class="s">/opt/prometheus/prometheus </span>\
<span class="s">  --config.file /opt/prometheus/config.yaml</span>

<span class="k">[Install]</span>
<span class="na">WantedBy</span><span class="o">=</span><span class="s">multi-user.target</span>
</code></pre></div><p>alertmanager</p>
<div class="highlight"><pre class="chroma"><code class="language-service" data-lang="service"><span class="k">[Unit]</span>
<span class="na">Description</span><span class="o">=</span><span class="s">Alertmanager Server</span>
<span class="na">Wants</span><span class="o">=</span><span class="s">network-online.target</span>
<span class="na">After</span><span class="o">=</span><span class="s">network-online.target</span>

<span class="k">[Service]</span>
<span class="na">Type</span><span class="o">=</span><span class="s">simple</span>
<span class="na">ExecStartPre</span><span class="o">=</span><span class="s">/opt/alertmanager/amtool check-config /opt/alertmanager/config.yaml</span>
<span class="na">ExecStart</span><span class="o">=</span><span class="s">/opt/alertmanager/altermanager </span>\
<span class="s">  --config.file=/opt/alertmanager/config.yaml </span>\
<span class="s">  --log.level=debug </span>\
<span class="s">  --cluster.listen-address=&#34;&#34;</span>

<span class="k">[Install]</span>
<span class="na">WantedBy</span><span class="o">=</span><span class="s">multi-user.target</span>
</code></pre></div><p>安装 node-exporter 和 kube-state-metrics。</p>
<blockquote>
<p>在 k3s 中我使用 helmchart 安装，但是这个 CRD 有一些问题，可以更换到 <a href="https://github.com/fluxcd/helm-operator">helm-operator</a>。</p>
</blockquote>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">helm.cattle.io/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HelmChart</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-node-exporter</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">helmVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v3</span><span class="w">
</span><span class="w">  </span><span class="nt">chart</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-node-exporter</span><span class="w">
</span><span class="w">  </span><span class="nt">targetNamespace</span><span class="p">:</span><span class="w"> </span><span class="l">monitoring</span><span class="w">
</span><span class="w">  </span><span class="nt">repo</span><span class="p">:</span><span class="w"> </span><span class="l">https://apphub.aliyuncs.com</span><span class="w">
</span><span class="w"></span><span class="nn">---</span><span class="w">
</span><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">helm.cattle.io/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HelmChart</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">kube-state-metrics</span><span class="w">
</span><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">helmVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v3</span><span class="w">
</span><span class="w">  </span><span class="nt">chart</span><span class="p">:</span><span class="w"> </span><span class="l">kube-state-metrics</span><span class="w">
</span><span class="w">  </span><span class="nt">targetNamespace</span><span class="p">:</span><span class="w"> </span><span class="l">monitoring</span><span class="w">
</span><span class="w">  </span><span class="nt">repo</span><span class="p">:</span><span class="w"> </span><span class="l">https://apphub.aliyuncs.com</span><span class="w">
</span></code></pre></div><p>安装两个 exporter 之后。</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@bowser1704 ~<span class="o">]</span><span class="c1"># kubectl get svc -n monitoring</span>
NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>    AGE
kube-state-metrics         ClusterIP   10.43.204.99    &lt;none&gt;        8080/TCP   19h
prometheus-node-exporter   ClusterIP   10.43.222.110   &lt;none&gt;        9100/TCP   39h
</code></pre></div><h2 id="单机监控中心监控多集群">单机监控中心监控多集群</h2>
<p>由标题，想要单机 prometheus 监控多个集群，不考虑单机压力问题，prometheus 配置和 exportor 的安装需要考虑两个问题：</p>
<ol>
<li>prometheus 通过公网访问集群的 metrics。</li>
<li>不使用 <a href="https://prometheus.io/docs/prometheus/latest/federation/#federation">prometheus federation</a> 或者 <a href="https://thanos.io/">thanos</a> 采集多集群的信息。</li>
</ol>
<h3 id="1-公网访问集群的-metrics">1. 公网访问集群的 metrics</h3>
<p>集群内部已经自建了一些 metrics 指标，这种指标可以直接通过它们暴露的 API 访问，例如 kubelet metrics，cAdvisor，API server&hellip;.。也可以通过 API server 提供的 proxy 访问。无论是怎么访问，从集群外部访问集群内部都需要手动配置 Authorization 可以直接通过 Bearer Token，也可以通过账号和密码。这里使用 token。</p>
<p>首先使用 <a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/">rbac</a> 创建一个 serviceaccount，并且创建 clusterrole，和做 clusterbinding。</p>
<p>可以参考我的 <a href="https://gist.github.com/Bowser1704/63d982782a3a8645316669d3100d8fc1">gist</a>。</p>
<blockquote>
<p>gist 中的 sa 并不能直接使用，需要后面进行二次修改，否则没有权限访问。</p>
<p>使用 clusterrole 需要访问集群所有信息，不考虑某个 namespace。</p>
</blockquote>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># 假设创建的 serviceaccount 是 monitoring 下的 prometheus</span>
kubectl get sa prometheus -n monitoring -o yaml

<span class="c1"># secret 的名字是上面获取的</span>
kubectl describe secret prometheus-token-cw4pd -n monitoring
</code></pre></div><h4 id="11-使用原生-api-访问内建指标">1.1 使用原生 API 访问内建指标</h4>
<blockquote>
<p>因为需要使用 https 但是证书是自签的，所以需要自己拿 ca.crt 下来，但是为了方便直接 disable validate certificates 了。</p>
</blockquote>
<ul>
<li>
<p>kubelet</p>
<p>Kubelet 监听于 10250 端口，暴露的 API 为 https://public-ip:10250/metircs，需要给 sa 的 clusterrole 设置一定的权限从而可以访问这个 API。我们使用上面创建的 sa，并且这个 sa 的 role 没有什么权限，假设只有一个 pod get 的权限，访问 API，结果是：</p>
<div class="highlight"><pre class="chroma"><code class="language-http" data-lang="http"><span class="err">Forbidden (user=system:serviceaccount:monitoring:prometheus, verb=get, resource=nodes, subresource=metrics)
</span></code></pre></div><p>说明这个 API 需要的权限是</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml">- <span class="nt">apiGroups</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="s2">&#34;&#34;</span><span class="w">
</span><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="l">nodes</span><span class="w">
</span><span class="w">  </span>- <span class="l">nodes/metrics</span><span class="w">
</span><span class="w">  </span><span class="nt">verbs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="l">get</span><span class="w">
</span></code></pre></div><p>在我们的 ClusterRole 里面加上这个权限。然后就可以拿到数据了，因为 http 的<a href="https://zh.wikipedia.org/wiki/%E5%88%86%E5%9D%97%E4%BC%A0%E8%BE%93%E7%BC%96%E7%A0%81">分块传输编码</a>，以及数据量很大，需要等待较长一段时间。</p>
<p>而在 prometheus.yml 里面的设置为。</p>
<blockquote>
<p>最下面的 relabel_configs 是把自带的 label 全都拿过来。</p>
</blockquote>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="c"># 我们的场景需要设置两个 tls_config 和 bearer_token。</span><span class="w">
</span><span class="w"></span>- <span class="nt">job_name</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;kubelet&#39;</span><span class="w">
</span><span class="w">    </span><span class="nt">scheme</span><span class="p">:</span><span class="w"> </span><span class="l">https</span><span class="w">
</span><span class="w">    </span><span class="nt">tls_config</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">insecure_skip_verify</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">bearer_token_file</span><span class="p">:</span><span class="w"> </span><span class="l">/var/run/secrets/kubernetes.io/serviceaccount/token</span><span class="w">
</span><span class="w">    </span><span class="nt">static_configs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">target</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">public-ip:10250</span><span class="w">
</span><span class="w">    </span><span class="nt">relabel_configs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">labelmap</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">__meta_kubernetes_node_label_(.+)</span><span class="w">
</span></code></pre></div></li>
<li>
<p>cAdvisor</p>
<p>由于高版本的 cAdvisor 合并到了 kubelet 里面，并且经过实践权限不需要修改，所以可以可以直接添加 prometheus job 就可以了。暴露的 API 为 https://public-ip:10250/metircs/cadvisor。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml">- <span class="nt">job_name</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;cAdvisor&#39;</span><span class="w">
</span><span class="w">    </span><span class="nt">scheme</span><span class="p">:</span><span class="w"> </span><span class="l">https</span><span class="w">
</span><span class="w">    </span><span class="nt">tls_config</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">insecure_skip_verify</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">    </span><span class="nt">bearer_token_file</span><span class="p">:</span><span class="w"> </span><span class="l">/var/run/secrets/kubernetes.io/serviceaccount/token</span><span class="w">
</span><span class="w">    </span><span class="nt">static_configs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">target</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">public-ip:10250</span><span class="w">
</span><span class="w">    </span><span class="nt">relabel_configs</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">labelmap</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">__meta_kubernetes_node_label_(.+)</span><span class="w">
</span><span class="w">    </span>- <span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">__meta_kubernetes_node_name]</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.+)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">__metrics_path__</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">metrics/cadvisor</span><span class="w">
</span></code></pre></div><blockquote>
<p>最下面的 relabel_configs 是把内建的  <code>__metrics_path__</code> 修改为 metircs/cadvisor 默认为 metrics。</p>
</blockquote>
</li>
<li>
<p>&hellip;&hellip;</p>
</li>
</ul>
<h4 id="12-使用-api-server-代理访问到内建指标">1.2 使用 API server 代理访问到内建指标</h4>
<p>根据 kubernetes 的架构，我们是通过 API server 操纵访问集群内的所有 API 对象的，并且我们也可以通过 API server 代理到我们的 service，pod 从而达到外网访问集群内部资源的效果。</p>
<p>这里还要介绍一个 prometheus config <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config">kubernetes_sd_config</a>，请参考文档详细配置信息。</p>
<blockquote>
<p>Kubernetes SD configurations allow retrieving scrape targets from <a href="https://kubernetes.io/">Kubernetes'</a> REST API and always staying synchronized with the cluster state.</p>
</blockquote>
<p>也就是说设置 kubernetes_sd_config 可以帮助我们自动发现需要监控的指标，可以根据集群的状态变化，例如你加了一个 node 他可能多了几个指标，这个时候就不需要我们手动去设置了，也就是实现了 service discovery。</p>
<ul>
<li>
<p>kubelet</p>
<p>账号依然是 1.1 中加了权限后的账号。token 还是那个 token， 证书可选。</p>
<p>下面是 config。我在 relabel_configs 主要做了几点配置：</p>
<ol>
<li>
<p>把 discover 发现的 pod ip，修改为 API server 暴露的地址 / master node public ip。</p>
<blockquote>
<p>prometheus 找到并且使用的都是 pod ip / endpoints。</p>
</blockquote>
</li>
<li>
<p>根据每个 node 的名字构建 metrics path，因为 kubelet metrics 每个 node 都有一个，所以需要采集多个 node 的 metircs，利用 kubernetes_sd_config 可以自动发现，不需要手动填写。</p>
</li>
</ol>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml">- <span class="nt">job_name</span><span class="p">:</span><span class="w"> </span><span class="l">stage-cadvisor</span><span class="w">
</span><span class="w">  </span><span class="nt">honor_timestamps</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">scrape_interval</span><span class="p">:</span><span class="w"> </span><span class="l">15s</span><span class="w">
</span><span class="w">  </span><span class="nt">scrape_timeout</span><span class="p">:</span><span class="w"> </span><span class="l">15s</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics_path</span><span class="p">:</span><span class="w"> </span><span class="l">/metrics</span><span class="w">
</span><span class="w">  </span><span class="nt">scheme</span><span class="p">:</span><span class="w"> </span><span class="l">https</span><span class="w">
</span><span class="w">  </span><span class="nt">kubernetes_sd_configs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">api_server</span><span class="p">:</span><span class="w"> </span><span class="l">https://:6443</span><span class="w">
</span><span class="w">    </span><span class="nt">role</span><span class="p">:</span><span class="w"> </span><span class="l">node</span><span class="w">
</span><span class="w">    </span><span class="nt">bearer_token_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/token</span><span class="w">
</span><span class="w">    </span><span class="nt">tls_config</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">ca_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/ca.crt</span><span class="w">
</span><span class="w">      </span><span class="nt">insecure_skip_verify</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">bearer_token_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/token</span><span class="w">
</span><span class="w">  </span><span class="nt">tls_config</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">ca_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/ca.crt</span><span class="w">
</span><span class="w">    </span><span class="nt">insecure_skip_verify</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">relabel_configs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">__meta_kubernetes_node_label_(.+)</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">$1</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">labelmap</span><span class="w">
</span><span class="w">  </span>- <span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.*)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">__address__</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">public-ip:6443</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span><span class="w">  </span>- <span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">__meta_kubernetes_node_name]</span><span class="w">
</span><span class="w">    </span><span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.+)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">__metrics_path__</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">/api/v1/nodes/${1}/proxy/metrics</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span></code></pre></div></li>
<li>
<p>cAdvisor</p>
<p>cAdivisor 和 kubelet 类似，只需要在 metrics path 后面加一个 cadvisor 就可以了。</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml">- <span class="nt">job_name</span><span class="p">:</span><span class="w"> </span><span class="l">stage-cadvisor</span><span class="w">
</span><span class="w">  </span><span class="nt">honor_timestamps</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">scrape_interval</span><span class="p">:</span><span class="w"> </span><span class="l">15s</span><span class="w">
</span><span class="w">  </span><span class="nt">scrape_timeout</span><span class="p">:</span><span class="w"> </span><span class="l">15s</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics_path</span><span class="p">:</span><span class="w"> </span><span class="l">/metrics</span><span class="w">
</span><span class="w">  </span><span class="nt">scheme</span><span class="p">:</span><span class="w"> </span><span class="l">https</span><span class="w">
</span><span class="w">  </span><span class="nt">kubernetes_sd_configs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">api_server</span><span class="p">:</span><span class="w"> </span><span class="l">https://public-ip:6443</span><span class="w">
</span><span class="w">    </span><span class="nt">role</span><span class="p">:</span><span class="w"> </span><span class="l">node</span><span class="w">
</span><span class="w">    </span><span class="nt">bearer_token_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/token</span><span class="w">
</span><span class="w">    </span><span class="nt">tls_config</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">ca_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/ca.crt</span><span class="w">
</span><span class="w">      </span><span class="nt">insecure_skip_verify</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">bearer_token_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/token</span><span class="w">
</span><span class="w">  </span><span class="nt">tls_config</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">ca_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/ca.crt</span><span class="w">
</span><span class="w">    </span><span class="nt">insecure_skip_verify</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">relabel_configs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">__meta_kubernetes_node_label_(.+)</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">$1</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">labelmap</span><span class="w">
</span><span class="w">  </span>- <span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.*)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">__address__</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">public-ip:6443</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span><span class="w">  </span>- <span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">__meta_kubernetes_node_name]</span><span class="w">
</span><span class="w">    </span><span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.+)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">__metrics_path__</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">/api/v1/nodes/${1}/proxy/metrics/cadvisor</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span><span class="w">  </span>- <span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.*)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">cluster</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">stage</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span></code></pre></div></li>
<li>
<p>apiserver</p>
<p>apiserver 可以直接通过 API server 暴露的地址加上一个 metrics path 就可以了，例如 https://public-ip:6443/metrics。</p>
<p>我在 relabel_configs 主要做的配置是：</p>
<ol>
<li>把 discover 发现的 pod ip，修改为 API server 暴露的地址 / master node public ip。</li>
</ol>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml">- <span class="nt">job_name</span><span class="p">:</span><span class="w"> </span><span class="l">stage-apiservers</span><span class="w">
</span><span class="w">  </span><span class="nt">honor_timestamps</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">scrape_interval</span><span class="p">:</span><span class="w"> </span><span class="l">15s</span><span class="w">
</span><span class="w">  </span><span class="nt">scrape_timeout</span><span class="p">:</span><span class="w"> </span><span class="l">15s</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics_path</span><span class="p">:</span><span class="w"> </span><span class="l">/metrics</span><span class="w">
</span><span class="w">  </span><span class="nt">scheme</span><span class="p">:</span><span class="w"> </span><span class="l">https</span><span class="w">
</span><span class="w">  </span><span class="nt">kubernetes_sd_configs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">api_server</span><span class="p">:</span><span class="w"> </span><span class="l">https://public-ip:6443</span><span class="w">
</span><span class="w">    </span><span class="nt">role</span><span class="p">:</span><span class="w"> </span><span class="l">endpoints</span><span class="w">
</span><span class="w">    </span><span class="nt">bearer_token_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/token</span><span class="w">
</span><span class="w">    </span><span class="nt">tls_config</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">insecure_skip_verify</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">bearer_token_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/token</span><span class="w">
</span><span class="w">  </span><span class="nt">tls_config</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">insecure_skip_verify</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">relabel_configs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]</span><span class="w">
</span><span class="w">    </span><span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">default;kubernetes;https</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">$1</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">keep</span><span class="w">
</span><span class="w">  </span>- <span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.*)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">__address__</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">public-ip:6443</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span></code></pre></div></li>
</ul>
<h4 id="13-使用-api-server-代理访问到自建指标">1.3 使用 API server 代理访问到自建指标</h4>
<p>你可以将自建的 exporter 都部署到一个 namespace 然后根据这个 namespace 的名字抓取所有的 metrics，或者根据某个 annotation 选择，但是这样的坏处是，所有的指标都在一个 job 下面，不太美观，出了问题不能一样排查出来，所以我选择的是 node-exporter，kube-state-metrics，traefik 都分开来。</p>
<ul>
<li>
<p>Node-exporter</p>
<p>node-exporter 会根据 node 的个数自己变化，所以我们肯定需要使用 kubernetes_sd_config 来自动发现。</p>
<p>下面是 config，注意我们 kubernetes_sd_config 的 role 是 endpoints。在 relabel_configs 做的配置是</p>
<ol>
<li>
<p>匹配 __meta_kubernetes_endpoints_name 为 prometheus-node-exporter 的，实际上这里是你 node-exporter service 的 name。</p>
<blockquote>
<p>你依然可以指定 namespace，指定 annotation，但是如果确定这个已经是唯一定位到你需要的 svc 就不需要写那么多了。</p>
</blockquote>
</li>
<li>
<p>更换 metrics path</p>
<p>目标为 <code>/api/v1/namespaces/$1/services/$2:$3/proxy$4</code>。</p>
</li>
</ol>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml">- <span class="nt">job_name</span><span class="p">:</span><span class="w"> </span><span class="l">stage-node-exporter</span><span class="w">
</span><span class="w">  </span><span class="nt">honor_timestamps</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">scrape_interval</span><span class="p">:</span><span class="w"> </span><span class="l">15s</span><span class="w">
</span><span class="w">  </span><span class="nt">scrape_timeout</span><span class="p">:</span><span class="w"> </span><span class="l">15s</span><span class="w">
</span><span class="w">  </span><span class="nt">metrics_path</span><span class="p">:</span><span class="w"> </span><span class="l">/metrics</span><span class="w">
</span><span class="w">  </span><span class="nt">scheme</span><span class="p">:</span><span class="w"> </span><span class="l">https</span><span class="w">
</span><span class="w">  </span><span class="nt">kubernetes_sd_configs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">api_server</span><span class="p">:</span><span class="w"> </span><span class="l">https://public-ip:6443</span><span class="w">
</span><span class="w">    </span><span class="nt">role</span><span class="p">:</span><span class="w"> </span><span class="l">endpoints</span><span class="w">
</span><span class="w">    </span><span class="nt">bearer_token_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/token</span><span class="w">
</span><span class="w">    </span><span class="nt">tls_config</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">insecure_skip_verify</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">bearer_token_file</span><span class="p">:</span><span class="w"> </span><span class="l">/opt/prometheus/serviceaccount/stage/token</span><span class="w">
</span><span class="w">  </span><span class="nt">tls_config</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">insecure_skip_verify</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span><span class="w">  </span><span class="nt">relabel_configs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">__meta_kubernetes_endpoints_name]</span><span class="w">
</span><span class="w">    </span><span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-node-exporter</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">$1</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">keep</span><span class="w">
</span><span class="w">  </span>- <span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">__meta_kubernetes_pod_annotation_prometheus_io_port]</span><span class="w">
</span><span class="w">    </span><span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(\d+)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">__meta_kubernetes_pod_container_port_number</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">$1</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span><span class="w">  </span>- <span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">__meta_kubernetes_service_annotation_prometheus_io_path]</span><span class="w">
</span><span class="w">    </span><span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">()</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">__meta_kubernetes_service_annotation_prometheus_io_path</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">/metrics</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span><span class="w">  </span>- <span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_pod_container_port_number, __meta_kubernetes_service_annotation_prometheus_io_path]</span><span class="w">
</span><span class="w">    </span><span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.+);(.+);(.+);(.+)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">__metrics_path__</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">/api/v1/namespaces/$1/services/$2:$3/proxy$4</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span><span class="w">  </span>- <span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.*)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">__address__</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">public-ip:6443</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span><span class="w">  </span>- <span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">__meta_kubernetes_service_label_(.+)</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">$1</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">labelmap</span><span class="w">
</span><span class="w">  </span>- <span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">__meta_kubernetes_namespace]</span><span class="w">
</span><span class="w">    </span><span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.*)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes_namespace</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">$1</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span><span class="w">  </span>- <span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">__meta_kubernetes_service_name]</span><span class="w">
</span><span class="w">    </span><span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.*)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes_name</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">$1</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span><span class="w">  </span>- <span class="nt">source_labels</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">__meta_kubernetes_pod_node_name]</span><span class="w">
</span><span class="w">    </span><span class="nt">separator</span><span class="p">:</span><span class="w"> </span><span class="l">;</span><span class="w">
</span><span class="w">    </span><span class="nt">regex</span><span class="p">:</span><span class="w"> </span><span class="l">(.*)</span><span class="w">
</span><span class="w">    </span><span class="nt">target_label</span><span class="p">:</span><span class="w"> </span><span class="l">instance</span><span class="w">
</span><span class="w">    </span><span class="nt">replacement</span><span class="p">:</span><span class="w"> </span><span class="l">$1</span><span class="w">
</span><span class="w">    </span><span class="nt">action</span><span class="p">:</span><span class="w"> </span><span class="l">replace</span><span class="w">
</span></code></pre></div></li>
</ul>
<p>其他的自建指标都与这个类似，使用 label 可以唯一找到目标就行了。</p>
<p>同时有些指标可能不需要服务发现，可能只有一个 endpoint，那我们其实可以手动写 static_config。</p>
<ul>
<li>kube-state-metrics</li>
<li>traefik</li>
<li>CoreDNS</li>
</ul>
<p>完整 config 可以参考 <a href="https://gist.github.com/Bowser1704/9ceb310cf9d0f1f605a5eeafb6ddbce4">gist</a></p>
<h3 id="2-监控多集群">2. 监控多集群</h3>
<p>由于 Prometheus 监控的单位是 job，并且一个 job 里面不能写多个 kubernetes_sd_config，但是 static_configs 里面可以有多个 target。所以说我们有两种思路：</p>
<ol>
<li>
<p>需要使用 kubernetes_sd_config 的，每个集群新建一个 job，而其他则可以使用 static_configs 并且使用 label 来区分。</p>
<p>例如：</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">static_configs</span><span class="p">:</span><span class="w">
</span><span class="w">  </span>- <span class="nt">targets</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">stage-ip:6443</span><span class="w">
</span><span class="w">    </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">cluster</span><span class="p">:</span><span class="w"> </span><span class="l">stage</span><span class="w">
</span><span class="w">  </span>- <span class="nt">targets</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="l">product-ip:6443</span><span class="w">
</span><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">cluster</span><span class="p">:</span><span class="w"> </span><span class="l">product</span><span class="w">
</span></code></pre></div></li>
<li>
<p>每个集群都新建所有 job，这个比较简单，有了一个模版，改字段就可以了，我上面的 config 就是这样，如果是 product1 集群，这 Crtl+R 替换所有的 stage 为 product1。</p>
<blockquote>
<p>stage、product 等名字都是自己定的。</p>
</blockquote>
<p>也不需要再添加新的 label，因为 job_name 就可以作为一个 label。</p>
</li>
</ol>
<p>最后的结果也是汇总到同一个 time series 下面的。</p>
<h2 id="prometheus-operator">Prometheus Operator</h2>
<p>目前来说集群的主流监控方案，使用的都是 <a href="https://prometheus.io/">prometheus</a> 组件，并且配合 <a href="https://grafana.com/">Grafana</a> 实现可视化。而随着 <a href="https://coreos.com/blog/introducing-operators.html">Operators</a> 的提出，以及 CoreOS 首先实现的两个 Operator 其中之一是 <a href="https://coreos.com/blog/the-prometheus-operator.html">Prometheus Operator</a>，主流的部署方案也不再是原生写 yaml 文件来部署 prometheus 等软件了，而是采用 CoreOS 提供的 <a href="https://github.com/coreos/prometheus-operator">Prometheus Operator</a> 部署，另外由于 Helm 的流行，更多的人使用 Helm 来安装 Prometheus Operator 从而使得集群监控方案的部署就像普通 Linux 发行版安装一个软件时使用一条包管理器命令一样简单。</p>
<blockquote>
<p>To demonstrate the Operator concept in running code, we have two concrete examples to announce as open source projects today:</p>
<ol>
<li>The <a href="https://coreos.com/blog/introducing-the-etcd-operator.html"><em>etcd Operator</em></a> creates, configures, and manages etcd clusters. etcd is a reliable, distributed key-value store introduced by CoreOS for sustaining the most critical data in a distributed system, and is the primary configuration datastore of Kubernetes itself.</li>
<li>The <a href="https://coreos.com/blog/the-prometheus-operator.html"><em>Prometheus Operator</em></a> creates, configures, and manages Prometheus monitoring instances. Prometheus is a powerful monitoring, metrics, and alerting tool, and a Cloud Native Computing Foundation (CNCF) project supported by the CoreOS team.</li>
</ol>
<p>&mdash; CoreOS</p>
</blockquote>
<h2 id="为什么不使用原生-prometheus">为什么不使用原生 Prometheus</h2>
<p>众所周知，目前来说，k8s 对于有状态（stateful）应用的管理还是没有很好，可以说是一个痛点，所以像 databases, caches, 和 monitoring systems 这些有状态应用，手动使用 StatefulSet 来部署，不过显然是没有 Operator 来的方便。</p>
<blockquote>
<p>operater 可以看成一个特定应用的 SRE 保姆。</p>
</blockquote>
<p>并且使用 Operator 将很多复杂的配置都抽离出来了。使部署一些需要高运维的工具不再那么容易。不过我们的场景用不了 Operator。</p>
]]></content>
		</item>
		
		<item>
			<title>k8s vxlan 作为 cni 后端引发的 63 秒延迟</title>
			<link>https://bowser1704.github.io/posts/vxlan-bug/</link>
			<pubDate>Sat, 27 Jun 2020 13:21:45 +0800</pubDate>
			
			<guid>https://bowser1704.github.io/posts/vxlan-bug/</guid>
			<description>vxlan as the flannel backend</description>
			<content type="html"><![CDATA[<p>使用 flannel 作为 cni 插件，并且使用 vxlan 作为后端会有 bug，表现为</p>
<ol>
<li>
<p>「 node 不能访问 pod 被调度到其他 node 的 service，但是可以通过 pod real IP/ endpoint 访问。」</p>
</li>
<li>
<p>实际上是「node 通过 svc 访问 调度到其他  node 的 pod 会有延时， 63s」</p>
</li>
</ol>
<blockquote>
<p>Flatcar / newer Linux kernels  延迟为 1 s</p>
</blockquote>
<h2 id="1-遇到-bug使用-cert-manager">1. 遇到 bug：使用 cert-manager</h2>
<p>首先是在使用 cert-manager 的时候</p>
<blockquote>
<p>cert-manager 的架构需要使用到一个 cert-manager-webhook 用来声明「创建一个 CR certificate」。但是如果 cert-manager-webhoook 被调度到所在的 node 之外的 node，就会出现 time-out。</p>
</blockquote>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">Error from server <span class="o">(</span>InternalError<span class="o">)</span>: error when creating <span class="s2">&#34;certificate.yaml&#34;</span>: Internal error occurred: failed calling webhook <span class="s2">&#34;webhook.cert-manager.io&#34;</span>: Post https://cert-manager-webhook.cert-manager.svc:443/mutate?timeout<span class="o">=</span>30s: dial tcp 10.43.18.211:443: i/o timeout
</code></pre></div><p>如上，因为 cert-manager 设置了一个 timeout 所以结果肯定是失败的。</p>
<ul>
<li>涉及 issue <a href="https://github.com/jetstack/cert-manager/issues/2811">https://github.com/jetstack/cert-manager/issues/2811</a></li>
</ul>
<h2 id="2-探寻原因">2. 探寻原因</h2>
<table>
<thead>
<tr>
<th style="text-align:center">源</th>
<th style="text-align:center">目标</th>
<th style="text-align:center">结果</th>
<th style="text-align:center">组别</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">nodeA</td>
<td style="text-align:center">service(pod in nodeA)</td>
<td style="text-align:center">success</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">nodeA</td>
<td style="text-align:center">service(pod in nodeB)</td>
<td style="text-align:center">failure/delay 63s</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">pod(in nodeA)</td>
<td style="text-align:center">service(pod in nodeB)</td>
<td style="text-align:center">success</td>
<td style="text-align:center">3</td>
</tr>
</tbody>
</table>
<p>参考 <a href="https://github.com/jetstack/cert-manager/issues/2811">cert-manager#2811</a>，也许修改 flannel backend to host-gw 可能有用。</p>
<blockquote>
<p>I changed the flannel backend from vxlan to host-gw, but it doesn&rsquo;t seem to work.</p>
</blockquote>
<ul>
<li>怎么修改 flannel backend</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">vim /etc/systemd/system/k3s.service
</code></pre></div><ul>
<li>
<p>修改的部分</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="nv">ExecStart</span><span class="o">=</span>/usr/local/bin/k3s server --flannel-backend host-gw
</code></pre></div></li>
<li>
<p>restart k3s</p>
</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">systemctl daemon-reload
systemctl restart k3s
<span class="c1">#如果没重启的话是不会的，所以要重启所有 node 「包括 agent」。</span>
</code></pre></div><ul>
<li>check my k3s net-conf 这样只能看到 backed 的设置。</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">- 检查每个节点的 flannel backend type。

​<span class="sb">```</span>bash
<span class="o">[</span>root@bowser1704 ~<span class="o">]</span><span class="c1"># kubectl get node bowser1704 -o yaml | grep backend-type</span>
    flannel.alpha.coreos.com/backend-type: host-gw
</code></pre></div><p><del>所以对我来说 更换为 host-gw 似乎并没有作用。</del></p>
<p>因为使用的是阿里云的云企业网，并且使用不同账号 vpc，属于跨 vpc，也就是阿里云的 vpc 网络是二层隔离的，所以 host-gw 不能使用。因为阿里云在中间做了一些拦截，把使用 node ip 作为路由下一跳的包全都拦了。</p>
<h2 id="3-着手-service检查-iptables-规则">3. 着手 service：检查 iptables 规则</h2>
<p>众所周知，service cluster IP 并不是真实 IP，而是通过 iptables 或者 ipvs 修改内核 netfilter 规则，做一些 <a href="https://en.wikipedia.org/wiki/Network_address_translation#DNAT">DNAT</a>(Destination Network Address Translation)，将给定的协议 / 数据报转发到 endpoing 「实际上是 pod 的 real ip，建立在 overlay network 的」。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggadttipz9j30u01270ya.jpg" alt="FW-IDS-iptables-Flowchart-v2019-04-30-1"></p>
<p>由上图可以知道数据包在 netfilter 内的走向。「需要有一定 iptables 知识」</p>
<p>并且我们知道，svc cluster ip 需要的是 DNAT，所以是 NAT table 中的 PREROUTING 和 OUTPUT chain，我们检查 PREROUING chain 和 OUTPUT chain。「在我们的场景下，packet 只会走 OUTPUT 和 POSTROUTING chain」</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># check nat table PREROUTING chain</span>
<span class="o">[</span>root@bowser1704 be<span class="o">]</span><span class="c1"># iptables -t nat -n -v -L PREROUTING/OUTPUT</span>
Chain PREROUTING <span class="o">(</span>policy ACCEPT <span class="m">49</span> packets, <span class="m">3398</span> bytes<span class="o">)</span>
 pkts bytes target     prot opt in     out     <span class="nb">source</span>               destination
3193K  209M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */
1944K  142M CNI-HOSTPORT-DNAT  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL

<span class="c1"># check KUBE-SERIVCES chain / food is my service name</span>
<span class="o">[</span>root@bowser1704 be<span class="o">]</span><span class="c1"># iptables -t nat -n -v -L KUBE-SERVICES | grep food</span>
    <span class="m">2</span>   <span class="m">120</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.42.0.0/16         10.43.105.114        /* food/food-backend:http cluster IP */ tcp dpt:8080
    <span class="m">2</span>   <span class="m">120</span> KUBE-SVC-J4YWF6HICEDZUWTC  tcp  --  *      *       0.0.0.0/0            10.43.105.114        /* food/food-backend:http cluster IP */ tcp dpt:8080
    <span class="m">0</span>     <span class="m">0</span> KUBE-MARK-MASQ  tcp  --  *      *      !10.42.0.0/16         10.43.105.118        /* food/redis: cluster IP */ tcp dpt:7388
    <span class="m">0</span>     <span class="m">0</span> KUBE-SVC-OXGTRCQ72XOGLTBD  tcp  --  *      *       0.0.0.0/0            10.43.105.118        /* food/redis: cluster IP */ tcp dpt:7388

<span class="c1"># check food svc chain</span>
<span class="o">[</span>root@bowser1704 be<span class="o">]</span><span class="c1"># iptables -t nat -n -v -L  KUBE-SVC-J4YWF6HICEDZUWTC</span>
Chain KUBE-SVC-J4YWF6HICEDZUWTC <span class="o">(</span><span class="m">1</span> references<span class="o">)</span>
 pkts bytes target     prot opt in     out     <span class="nb">source</span>               destination
    <span class="m">2</span>   <span class="m">120</span> KUBE-SEP-Z45YJMXQTGNBLAMQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0

<span class="c1"># check kstack sep chain / sep is made for load balance</span>
<span class="o">[</span>root@bowser1704 be<span class="o">]</span><span class="c1"># iptables -t nat -n -v -L  KUBE-SEP-Z45YJMXQTGNBLAMQ</span>
Chain KUBE-SEP-Z45YJMXQTGNBLAMQ <span class="o">(</span><span class="m">1</span> references<span class="o">)</span>
 pkts bytes target     prot opt in     out     <span class="nb">source</span>               destination
    <span class="m">0</span>     <span class="m">0</span> KUBE-MARK-MASQ  all  --  *      *       10.42.1.4            0.0.0.0/0
    <span class="m">2</span>   <span class="m">120</span> DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:10.42.1.4:8080

<span class="o">[</span>root@bowser1704 be<span class="o">]</span><span class="c1"># iptables -t nat -n -v -L KUBE-MARK-MASQ</span>
Chain KUBE-MARK-MASQ <span class="o">(</span><span class="m">56</span> references<span class="o">)</span>
 pkts bytes target     prot opt in     out     <span class="nb">source</span>               destination
   <span class="m">58</span>  <span class="m">2400</span> MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK or 0x4000
</code></pre></div><p>而 POSTROUTING 做了什么呢？</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@bowser1704 ~<span class="o">]</span><span class="c1"># iptables -t nat -n -v -L POSTROUTING</span>
Chain POSTROUTING <span class="o">(</span>policy ACCEPT <span class="m">5747</span> packets, 614K bytes<span class="o">)</span>
 pkts bytes target     prot opt in     out     <span class="nb">source</span>               destination
4948K  362M CNI-HOSTPORT-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* CNI portfwd requiring masquerade */
4948K  362M KUBE-POSTROUTING  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */
2346K  150M RETURN     all  --  *      *       10.42.0.0/16         10.42.0.0/16
 <span class="m">2608</span>  198K MASQUERADE  all  --  *      *       10.42.0.0/16        !224.0.0.0/4
<span class="m">15770</span>  690K RETURN     all  --  *      *      !10.42.0.0/16         10.42.0.0/24
    <span class="m">3</span>   <span class="m">180</span> MASQUERADE  all  --  *      *      !10.42.0.0/16         10.42.0.0/16
<span class="o">[</span>root@bowser1704 ~<span class="o">]</span><span class="c1"># iptables -t nat -n -v -L CNI-HOSTPORT-MASQ</span>
Chain CNI-HOSTPORT-MASQ <span class="o">(</span><span class="m">1</span> references<span class="o">)</span>
 pkts bytes target     prot opt in     out     <span class="nb">source</span>               destination
    <span class="m">0</span>     <span class="m">0</span> MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            mark match 0x2000/0x2000
<span class="o">[</span>root@bowser1704 ~<span class="o">]</span><span class="c1"># iptables -t nat -n -v -L KUBE-POSTROUTING</span>
Chain KUBE-POSTROUTING <span class="o">(</span><span class="m">1</span> references<span class="o">)</span>
 pkts bytes target     prot opt in     out     <span class="nb">source</span>               destination
  <span class="m">448</span> <span class="m">18044</span> MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ mark match 0x4000/0x4000
</code></pre></div><p>也就是匹配加了 mark 0x4000/0x4000 的 packet，并做一次 SNAT。</p>
<p>kubu-proxy 用来实现 DNAT 和 SNAT 「k3s 二进制文件内置了，没有单独起一个进程。」</p>
<ul>
<li>
<p>实现 SNAT 是利用 masquerading rules 「与 SNAT rules 有一些区别，SNAT rules 转换的 IP 是给定的，但是这种方式是 动态获取当前 IP 的。」</p>
<p>实现 SNAT 只针对于，pod（in k3s it’s 10.42.0.0/16） 之外的网段访问 svc，具体过程是先用 KUBE-MARK-MASQ 加一个 mark，POSTROUING 阶段检查，如果有这个 mark 就做 SNAT。</p>
</li>
<li>
<p>实现 DNAT 是利用 DNAT rules。</p>
</li>
</ul>
<hr>
<p><strong>根据第二步中的对照组，</strong></p>
<ul>
<li>1 3 对照发现 node 和 pod in node 访问结果不一样，在这里区别是 node 做了 SNAT。所以 SNAT 可能有影响。</li>
</ul>
<h2 id="4-查看-dnat-之后的走向">4. 查看 DNAT 之后的走向</h2>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="o">[</span>root@bowser1704 ~<span class="o">]</span><span class="c1"># route -n</span>
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         172.19.159.253  0.0.0.0         UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> eth0
10.42.0.0       0.0.0.0         255.255.255.0   U     <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> cni0
10.42.1.0       10.42.1.0       255.255.255.0   UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> flannel.1
10.42.2.0       10.42.2.0       255.255.255.0   UG    <span class="m">0</span>      <span class="m">0</span>        <span class="m">0</span> flannel.1
</code></pre></div><hr>
<p><strong>根据第二步中的对照组</strong></p>
<p>1 2 对照可以发现，1 和 2 结果不一样。在这里不一样的原因是 1 中直接走 cni 了，但是 2 中会走 flannel。所以走 flannel 可能有影响，并且根据别人使用 host-gw 会有帮助，也许 vxlan 有问题。</p>
<blockquote>
<p>cni0 是容器网桥，flannel.1 是 cni 插件 flannel 网桥，走 flannel 意味着会有 vxlan。</p>
</blockquote>
<h2 id="5-追踪数据包-tcpdump-抓包">5. 追踪数据包 tcpdump 抓包</h2>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1"># 监听 pod ip / endpoing</span>
tcpdump -i any -vv host 10.42.1.34

<span class="c1"># 另一个 shell curl svc</span>
curl 10.43.105.114:8080/sd/health

<span class="c1"># P.S. 此时不能有其他人访问这个 pod。</span>
</code></pre></div><p>发现有输出，说明监听到了，也就是 svc 转发到了对应的 pod。<strong>并且等了一分钟左右他居然是能连通的</strong>。</p>
<p><strong>初次之外，建立 TCP 连接时，一直重发 SYN，第七次才真正传送到了。</strong></p>
<p>明确两点：</p>
<ul>
<li>tcp 数据报是转发到了 pod IP 上的。</li>
<li>63s 的延迟之后又可以了。</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggar4e5zmqj30q304n0te.jpg" alt="image-20200630235158877"></p>
<p>在 tcp 三次握手时：</p>
<ol>
<li>客户端首先要发送一个 SYN 给服务端</li>
<li>服务端再发送一个 SYN-ACK 回复客户端</li>
<li>客户端最后发送一个 ACK 给服务端，握手就成功了。</li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggahwt6l8vj30go0960tb.jpg" alt="img"></p>
<blockquote>
<p>当连接未建立成功，重传需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s。并且第六次重传输会取消 checksum，也就是 &lsquo;no cksum&rsquo; 。</p>
</blockquote>
<hr>
<p>根据 TCP 的原理以及前几步骤的确认，可以确认问题出现在 IP packet 进入 flannel 之后，并且做了 SNAT 以及 利用 VXLAN。</p>
<h2 id="6-追踪产生-bug-的原因">6. 追踪产生 bug 的原因</h2>
<p>由第三步可以暂时认定是有 SNAT 的原因。并且做 SNAT 转出来还是自己的 IP。</p>
<p>关于内核 SNAT 的步骤，参考 <a href="https://github.com/torvalds/linux/blob/24de3d377539e384621c5b8f8f8d8d01852dddc8/net/netfilter/nf_nat_core.c#L290-L301">Linux NAT core</a>。</p>
<ol>
<li>检查 source IP 是否为 IP pool 内的 IP，如果是的话直接返回。</li>
<li>找到 IP pool 里面最少使用的 IP，将 IP packet 的 source IP 换成这个 IP。</li>
<li>检查允许的端口，如果目前的端口，本来就空闲，就不变。之后再返回。</li>
<li>找一个用于 SNAT 的端口 by calling <a href="https://github.com/torvalds/linux/blob/24de3d377539e384621c5b8f8f8d8d01852dddc8/net/netfilter/nf_nat_proto_common.c#L37-L85">nf_nat_l4proto_unique_tuple()</a>。</li>
</ol>
<p>iptables 中的 KUBE-MARK-MASQ 用作标记要不要 SNAT，POSTROUING 会做一次 SNAT，并且 VXLAN 封装之后还会做一次 SNAT。</p>
<ul>
<li>
<p>flannel issue</p>
<p><a href="https://github.com/coreos/flannel/pull/1282#issuecomment-635639567">coreos/flannel#1282 (comment)</a></p>
<p><a href="https://github.com/coreos/flannel/pull/1282#issuecomment-635145841">coreos/flannel#1282 (comment)</a></p>
<p><a href="https://github.com/coreos/flannel/pull/1282#issuecomment-635145841">coreos/flannel#1282 (comment)</a></p>
<p>有人关于 &ndash;random-fully 参数做了详细的测试，这个参数是用于选择 SNAT 的两种算法。</p>
</li>
</ul>
<p>大意是 iptables 和 kube-proxy 对 &ndash;random-fully 支持的问题。</p>
<ul>
<li>
<p>k8s issue <a href="https://github.com/kubernetes/kubernetes/issues/88986#issuecomment-640929804">https://github.com/kubernetes/kubernetes/issues/88986#issuecomment-640929804</a></p>
<p>这个 issue 中的 comment 详细讲述的原因和如何去解决这个问题。</p>
</li>
</ul>
<p>上面这些原因是引起使用 VXLAN/VETH 时 incorrect checksum 的原因，并且是一起触发的。</p>
<ol>
<li>
<p>由于 packet mark 的方式，当我们在第一次对 packet mark 0x4000/0x4000 并且进行 SNAT 之后，进入 VETH，经过 cni 插件出来之后，POSTROUTING 仍然会识别到这个包的 mark，并且进行二次 SNAT。</p>
</li>
<li>
<p>并且因为 &ndash;random-fully SNAT 方式，source port 会改变，并且因为 kernel bug 不会进行第二次 check sum，所以最后的 checksum is incorrect，解决方案可以是禁止 double SNAT 所以不会有 bad checksum 的情况，或者是升级内核使得他会进行第二次 checksum。</p>
<p>kernel checksum bug <a href="https://tech.vijayp.ca/linux-kernel-bug-delivers-corrupt-tcp-ip-data-to-mesos-kubernetes-docker-containers-4986f88f7a19">参考链接</a></p>
</li>
</ol>
<hr>
<p>checksum-offload 指定了内核不做校验和，交给网卡 / 硬件去做，但是使用 VXLAN 时，由于上面某些原因校验值是错的。导致 checksum 错误，所以会丢弃，TCP 不得不重传，然后因为 5 次重传耗时 63s「第六次重传取消 checksum」，所以结果是 63s delay。</p>
<blockquote>
<p><a href="https://github.com/projectcalico/calico/issues/3145">https://github.com/projectcalico/calico/issues/3145</a></p>
<p>Linux’ TCP stack will sometimes/always attempt to send packets with an incorrect checksum, or that are far too large for the network link, with the result that the packet is rejected and TCP has to re-transmit. This slows down network throughput enormously.</p>
</blockquote>
<p>如下图，本级 checksum 一直 incorrect。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggatc17gefj31hc0i6n3x.jpg" alt="image-20200701010831331"></p>
<p>解决办法</p>
<ol>
<li>
<p>关闭 Checksum Offloading</p>
<p><code>ethtool -K flannel.1 tx-checksum-ip-generic off</code></p>
</li>
</ol>
<h2 id="7-fix-this-bug">7. fix this bug</h2>
<ol>
<li>
<p><a href="https://github.com/kubernetes/kubernetes/pull/92035#issuecomment-644502203">https://github.com/kubernetes/kubernetes/pull/92035#issuecomment-644502203</a></p>
<p>pr 中详细讲述了触发这个 bug 的几要素。他的方法是使得走 flannel 后不会校验失败。</p>
</li>
<li>
<p>关闭 Checksum Offloading</p>
<p>因为是用的是 flannel 等虚拟网络硬件，做 checksum 等操作会 incorrect，所以关闭其实还是最省心的。</p>
</li>
</ol>
<hr>
<p>参考链接</p>
<ul>
<li>iptables <a href="https://www.digitalocean.com/community/tutorials/a-deep-dive-into-iptables-and-netfilter-architecture">https://www.digitalocean.com/community/tutorials/a-deep-dive-into-iptables-and-netfilter-architecture</a></li>
<li>explaination for the same bug <a href="https://tech.xing.com/a-reason-for-unexplained-connection-timeouts-on-kubernetes-docker-abd041cf7e02">https://tech.xing.com/a-reason-for-unexplained-connection-timeouts-on-kubernetes-docker-abd041cf7e02</a></li>
<li>linux code <a href="https://github.com/torvalds/linux/blob/24de3d377539e384621c5b8f8f8d8d01852dddc8/net/netfilter/nf_nat_core.c#L290-L291">https://github.com/torvalds/linux/blob/24de3d377539e384621c5b8f8f8d8d01852dddc8/net/netfilter/nf_nat_core.c#L290-L291</a></li>
<li><a href="https://github.com/weaveworks/weave/issues/1255#issuecomment-221820171">weaveworks/weave#1255 (comment)</a></li>
<li>VXLAN <a href="https://community.mellanox.com/s/article/vxlan-considerations-for-connectx-3-pro">https://community.mellanox.com/s/article/vxlan-considerations-for-connectx-3-pro</a></li>
<li>checksum-bug <a href="https://tech.vijayp.ca/linux-kernel-bug-delivers-corrupt-tcp-ip-data-to-mesos-kubernetes-docker-containers-4986f88f7a19">https://tech.vijayp.ca/linux-kernel-bug-delivers-corrupt-tcp-ip-data-to-mesos-kubernetes-docker-containers-4986f88f7a19</a></li>
<li><a href="https://www.dynatrace.com/news/blog/detecting-network-errors-impact-on-services/">https://www.dynatrace.com/news/blog/detecting-network-errors-impact-on-services/</a></li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>谈谈系统启动发生了什么</title>
			<link>https://bowser1704.github.io/posts/system-boot/</link>
			<pubDate>Thu, 22 Aug 2019 16:23:18 +0800</pubDate>
			
			<guid>https://bowser1704.github.io/posts/system-boot/</guid>
			<description>&lt;p&gt;系统启动实际上东西比较多，这里只是讲一讲，系统引导方面的东西。&lt;/p&gt;</description>
			<content type="html"><![CDATA[<p>系统启动实际上东西比较多，这里只是讲一讲，系统引导方面的东西。</p>
<h3 id="0-杂谈">0. 杂谈</h3>
<p>首先我们知道在系统中启动叫做 <code>boot</code>，取自 Bootstrap，这里有个小故事</p>
<blockquote>
<p>Bootstrap 不是鞋带的意思，应该是“鞋子背带”的意思（http://en.wiktionary.org/wiki/bootstrap）
在这里隐喻表示一种不需要外部帮助自己能够处理事情的情形。“pull oneself up by one&rsquo;s bootstraps”最初来自于《The Surprising Adventures of Baron Munchausen》这本书里的一个故事：主人公 Baron Munchausen 不小心掉进了一片沼泽，他通过自己的 bootstraps 将自己拉了出来（当然有童话神奇的色彩）。事实上在 19 世纪初美国就有&quot;pull oneself over a fence by one&rsquo;s bootstraps&quot;的语言，意思是“做荒谬不可能完成的事情”。
参考：http://en.wikipedia.org/wiki/Bootstrapping</p>
</blockquote>
<p>为什么说这个故事呢，我们要启动系统，就要启动程序对吧，但是启动程序又要系统，这不就是一个死循环了吗？所以 <code>一种不需要外部帮助自己能够处理事情的情形</code>，在当时 ROM（Read only memory）的发展下，人们刚开始发明了 BIOS（Basic Input/Output System），后来又出现了 <code>UEFI</code> 全称“统一的可扩展固件接口”(Unified Extensible Firmware Interface)。</p>
<h3 id="1-最开始是如何加载系统的">1. 最开始是如何加载系统的</h3>
<h4 id="11-老一代的-legacy-bios--mbr">1.1 老一代的 Legacy BIOS + <code>MBR</code></h4>
<p>最初的启动就是按下电源键之后，电脑读取写入 ROM 的 BIOS。BIOS 开始下面几个步骤。</p>
<ol>
<li>硬件自检（Power-On Self-Test）简称为 POST，如果有问题计算机会发出不同含义的蜂鸣声。（有没有很傻屌）</li>
<li>选择启动顺序，现在 BIOS 开始要运行的权利给下一个 device 了，但是你电脑可能有多个硬盘，也可能你会插入 U 盘，所以可能需要你选择一个设备。
<ul>
<li>计算机开始读取设备的第一个扇区，也就是 512 字节，就叫做&quot;主引导记录&quot;（Master boot record，缩写为 <code>MBR</code>）。</li>
<li>但是 <code>MBR</code> 有一些问题，所以现在基本不用了。</li>
</ul>
</li>
<li>从硬盘启动，加载系统内核相关的东西。
<ul>
<li>这里因为 <code>MBR</code>，所以系统启动读取的是 <code>MBR</code> 内的启动程序，但是如果你一个硬盘，装了很多的系统，每次装新的系统，后面的启动代码就会覆盖前者的。</li>
</ul>
</li>
</ol>
<p>总结的话 <code>BIOS</code> 不认识设备，直接硬的来，你的 <code>MBR</code> 内写了什么，他就是什么。</p>
<h4 id="12-进入-linux-系统之后-initd">1.2 进入 Linux 系统之后 initd</h4>
<ol>
<li>加载内核，进入 <code>/boot</code> 下找到内核文件，加载。</li>
<li>开始运行初始化文件 <code>/sbin/init</code>，他的作用是初始化系统环境，<code>init</code> 就是第一个进程，<code>pid</code>=1</li>
<li>确定运行级别</li>
<li>加载开机启动程序</li>
<li>用户登录</li>
<li>进入 login shell</li>
<li>打开 non-login shell</li>
</ol>
<p>这里初略讲，详情看阮一峰的文章。</p>
<p>阮一峰写了两篇文章对于以前的启动方式很好的讲解了</p>
<p><a href="http://www.ruanyifeng.com/blog/2013/02/booting.html">http://www.ruanyifeng.com/blog/2013/02/booting.html</a> 参考文章，作者阮一峰，对于 <code>MBR</code> 的详细解释</p>
<p><a href="http://www.ruanyifeng.com/blog/2013/08/linux_boot_process.html">http://www.ruanyifeng.com/blog/2013/08/linux_boot_process.html</a> 参考文章，作者阮一峰，对于 <code>Linux</code> 内核启动的解释。</p>
<h3 id="2-今天我们怎么加载系统">2. 今天我们怎么加载系统</h3>
<h4 id="21-新时代的-uefi-biosgpt">2.1 新时代的 <code>UEFI BIOS</code>+<code>GPT</code></h4>
<p><code>UEFI</code> 不同于 legacy BIOS 的是，他认识设备，他会在 <code>/boot/efi</code> 寻找以 <code>.efi</code> 为后缀的文件，里面有一个目录 <code>EFI</code>，放了各种系统的 <code>efi</code> 启动程序。</p>
<p>例如（不同厂商的驱动放在不同厂家的文件夹下面）</p>
<blockquote>
<p>启动 windows	进入 Microsoft/Boot/*.efi</p>
<p>启动 <code>ubuntu</code>       进入 ubuntu/*.efi</p>
</blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/4072641-4b58e0e13c209d14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/345/format/webp" alt="efi"></p>
<h5 id="流程">流程</h5>
<ol>
<li>
<p>POST，硬件自检</p>
</li>
<li>
<p><code>UEFI</code> 固件加载，一系列初始化。</p>
</li>
<li>
<p>按照设置里面的顺序，读取 <code>efi</code> 启动项，加载硬件驱动，解析其中的分区表（GPT 和 MBR）。</p>
<p>启动项分为两种</p>
<ul>
<li>文件启动项，即 <code>UEFI</code> 已经记录了启动项的地址，某个磁盘-&gt; 某个分区-&gt;/<code>EFI/Boot/*.efi</code></li>
<li>设备启动项，就是磁盘，会直接去磁盘里面寻找 <code>ESP</code> 分区下的 <code>/EFI/Boot/*.efi</code></li>
</ul>
</li>
</ol>
<blockquote>
<p>随着 Windows8.x，以及 UEFI 标准 2.x，win 推出了一个叫做 SecureBoot 的功能。开了 SecureBoot 之后，主板会验证即将加载的 efi 文件的签名，如果开发者不是受信任的开发者，就会拒绝加载。
比如 CloverX64.efi 就好像没有签名。</p>
</blockquote>
<p>所以在装 Linux 的时候我们会关闭 SecureBoot，防止不被授权，无法加载 efi</p>
<h5 id="磁盘分区文件系统">磁盘分区，文件系统</h5>
<p>磁盘就是我们用的硬盘，U 盘之类的，我们要对磁盘进行分区，其实不分区可以理解为只分一个区，然后每个区要进行格式化，并且选择文件系统。不同类型磁盘有不同类型的设备驱动，不同系统有不同的文件系统驱动。</p>
<ul>
<li>磁盘驱动
<ul>
<li><code>win8/10</code> 含有 <code>IDE/SATA/NVME</code> 三种驱动</li>
<li><code>macos</code> 含有 <code>/SATA/NVME</code> 驱动，但是 10.13 之前是苹果专用 <code>NVME</code> 驱动</li>
</ul>
</li>
<li>文件系统驱动
<ul>
<li><code>win10</code> 含有 FAT32、NTFS、exFAT、ReFS 几种</li>
<li>Linux 含有 ext2、ext3、ext4、FAT32 等</li>
<li>macOS 含有 FAT32、HFS+、APFS、exFAT，NTFS 只读</li>
</ul>
</li>
</ul>
<blockquote>
<p>驱动是可以后期安装的，Paragon 这个公司推出了 NTFS for Mac、HFS for Windows、ExtFS for……等一套文件系统驱动。</p>
</blockquote>
<h5 id="总结">总结</h5>
<p>UEFI 规范里，在 GPT 分区表的基础上，规定了一个 EFI 系统分区（EFI System Partition，ESP），ESP 要格式化成 FAT32，EFI 启动文件要放在“/EFI&lt; 厂商 &gt;”文件夹下面。<strong>但是 Apple 比较特殊</strong></p>
<p>作为 UEFI 标准里，钦定的文件系统，FAT32.efi 是每个主板都会带的。所有 UEFI 的主板都认识 FAT32 分区。这就是为啥非得是 FAT32 的。</p>
<p>至于 GPT（GUID Partition Table，缩写：GPT），可以理解为新时代的 <code>MBR</code>，分区表。</p>
<p>所以说</p>
<blockquote>
<p>装系统可以不用制作启动盘了，将 iso 文件压缩到一个 FAT32 分区内，~~ 然后在将该分区下的 <code>EFI/Boot/BOOTx64</code> 添加到 UEFI 文件启动项 ~~，直接进入 UEFI 引导，就可以选择这一项启动。</p>
</blockquote>
<blockquote>
<p>另外 UEFI 为了兼容 MBR，是可以用的，但是 Lgacy 不支持 GPT</p>
</blockquote>
<h4 id="22-systemd-启动系统">2.2 Systemd 启动系统</h4>
<p>init 启动系统有几点不好</p>
<ul>
<li>串行启动，启动时间长</li>
<li>启动脚本复杂</li>
</ul>
<p>Systemd 是用来替代 init 启动而生的，读作 System daemon（系统，守护进程）。</p>
<p>Systemd 是一组命令，涉及到系统管理的方方面面，本来我们启动系统是用 initd 初始化一个 pid=1 的进程，然后所有进程都是他的子进程，但是现在的话我们不需要，直接启动 Systemd，用它来管理系统。</p>
<p>这个时候就和上面类似了。但是我们要重点关注 <code>systemd</code> 命令组。这里就会体现 Linux 下一切皆文件的思想。</p>
<p><img src="https://ccnupp.oss-cn-shanghai.aliyuncs.com/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6.svg" alt="未命名文件"></p>
<h4 id="这里还要提一点系统进入之后">这里还要提一点，系统进入之后</h4>
<p>首先我们会进入 login shell，这个时候会读取一些配置文件。</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash"><span class="c1">#命令行登录</span>
~/.bash_profile
~/.bash_login
~/.profile	<span class="c1">#这里有语句会同时执行.bashrc</span>
<span class="c1">#只要读取了上面一个，就不会再读取后面的了，按上述顺序读取。</span>

<span class="c1">#图形界面登录的话,只会读取下面两个，不管是否有.bash_profile 存在</span>
etc/ptofile	<span class="c1">#这个是对于所有用户的配置文件，~/.bashrc 是对于当前用户的。</span>
~/.profile
</code></pre></div><p>然后我们手动开启的 shell 叫做 non login shell，他就会读取 .bashrc</p>
<h3 id="参考文章">参考文章</h3>
<p><a href="https://www.ibm.com/developerworks/cn/linux/l-lpic1-101-2/index.html">https://www.ibm.com/developerworks/cn/linux/l-lpic1-101-2/index.html</a>	IBM，引导系统</p>
<p><a href="http://www.ruanyifeng.com/blog/2013/02/booting.html">http://www.ruanyifeng.com/blog/2013/02/booting.html</a> 参考文章，作者阮一峰，对于 MBR 的详细解释</p>
<p><a href="http://www.ruanyifeng.com/blog/2013/08/linux_boot_process.html">http://www.ruanyifeng.com/blog/2013/08/linux_boot_process.html</a> 参考文章，作者阮一峰，对于 Linux 内核启动的解释。</p>
<p>关于 systemd 使用，这里有几篇文章。</p>
<p><a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html">http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html</a>	作者：阮一峰</p>
<p><a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-part-two.html">http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-part-two.html</a>	作者：阮一峰</p>
<p><a href="https://wiki.archlinux.org/index.php/Systemd_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)">https://wiki.archlinux.org/index.php/Systemd_</a>	来源 Archwiki</p>]]></content>
		</item>
		
	</channel>
</rss>
